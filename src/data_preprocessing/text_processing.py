"""
This module provides text preprocessing utilities for NLP tasks. It includes
functions to clean text data, convert part-of-speech tags, lemmatize text, and
numericalize text based on a given vocabulary. These utilities are designed to
be used as part of a larger pipeline for preparing text data for machine
learning models.

The module includes:
- `clean_text`: Cleans input text by removing punctuation, stopwords, and
  converting to lowercase.
- `nltk_to_wordnet_pos`: Converts NLTK POS tags to WordNet POS tags for
  accurate lemmatization.
- `lemmatizer`: Lemmatizes text data using WordNetLemmatizer to reduce words
  to their base form.
- `numericalize`: Converts tokens in text data to numerical indices based on a
  given vocabulary. 
Dependencies:
- NLTK (Natural Language Toolkit) is used for tokenization, stopwords,
  POS tagging, and lemmatization.
"""

import re
# import os
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
import nltk
from typing import List, Dict, Union


nltk.data.path.append('nltk_data')
# nltk.data.load(os.path.join('nltk_data',
#                             'tokenizers/punkt/PY3/english.pickle'))
# nltk.data.load(
#     os.path.join(
#         'nltk_data',
#         'taggers',
#         'averaged_perceptron_tagger',
#         'averaged_perceptron_tagger.pickle'
#     )
# )
# nltk.data.load(os.path.join('nltk_data', 'corpora/stopwords/english'),
#                format='text')

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('stopwords')


def clean_text(text: str) -> str:
    """
    Clean text data by removing punctuation, stopwords, \
           and converting to lowercase.
    Args: text (str): Input text from the user to be cleaned.
    Returns (str): cleaned text
    """
    tokens = re.sub(r"\{\{.*?\}\}", "", text)
    tokens = word_tokenize(tokens)
    tokens = [w.lower() for w in tokens]
    tokens = [word for word in tokens if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)
  
  
def nltk_to_wordnet_pos(nltk_tag: str) -> str:
    """
    Converts an NLTK POS (part-of-speech) tag to a
    corresponding WordNet POS tag.
    This conversion is useful for lemmatization tasks where WordNet requires 
    specific POS tags.
    Args:
        nltk_tag (str): The POS tag generated by NLTK's `pos_tag` function.
    Returns:
        str or None: The corresponding WordNet POS tag (`wordnet.ADJ`,
                     `wordnet.VERB`, `wordnet.NOUN`, `wordnet.ADV`), or `None`
                     if the NLTK tag does not correspond to any of these.
    """
    if nltk_tag.startswith('J'):
        return wordnet.ADJ
    elif nltk_tag.startswith('V'):
        return wordnet.VERB
    elif nltk_tag.startswith('N'):
        return wordnet.NOUN
    elif nltk_tag.startswith('R'):
        return wordnet.ADV
    else:
        return None
      

def lemmatizer(data: str) -> List[str]:
    """
    Lemmatizes the input text data by converting words to their base
    or dictionary form using NLTK's WordNetLemmatizer.
    The function first tokenizes the input text, assigns 
    part-of-speech (POS) tags, and then lemmatizes each word 
    based on its POS tag.

    Args:
        data (str): The input text to be lemmatized.

    Returns:
        list: A list of lemmatized words from the input text.
    """
    wordnet_lem = WordNetLemmatizer()
    tokens = word_tokenize(data)
    pos_tags = pos_tag(tokens)

    lemmatized_words = []
    for word, tag in pos_tags:
        wn_tag = nltk_to_wordnet_pos(tag)
        if wn_tag is None:
            lemmatized_word = wordnet_lem.lemmatize(word)
        else:
            lemmatized_word = wordnet_lem.lemmatize(word, pos=wn_tag)
        lemmatized_words.append(lemmatized_word)

    return lemmatized_words
  

def numericalize(vocab: Dict[str, int], 
                 data: Union[List[str], List[List[str]]]) -> List[List[int]]:
    """
    Converts tokens into their corresponding numerical indices
    based on a given vocabulary. If a token is not found in the vocabulary,
    a special '<UNK>' token is used.
    Args:
        vocab (dict): A dictionary mapping tokens (words) to their
                      corresponding numerical indices.
        data (Union[List[str], List[List[str]]]): 
                     A list of tokens (words) to be converted into numerical
                     indices.
    Returns:
        List[List[int]]: A list containing a list of numerical indices 
                        representing the input text data.
    """
    indexed_data = []
    indexed_seq = [vocab.get(token, vocab['<UNK>']) for token in data]
    indexed_data.append(indexed_seq)

    return indexed_data
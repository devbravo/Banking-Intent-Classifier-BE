[
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "bentoml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "bentoml",
        "description": "bentoml",
        "detail": "bentoml",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "IntentClassifier",
        "importPath": "classifier_model",
        "description": "classifier_model",
        "isExtraImport": true,
        "detail": "classifier_model",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "classifier_model",
        "description": "classifier_model",
        "isExtraImport": true,
        "detail": "classifier_model",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "wordnet",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "pos_tag",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "torch.nn.init",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "Text",
        "importPath": "bentoml.io",
        "description": "bentoml.io",
        "isExtraImport": true,
        "detail": "bentoml.io",
        "documentation": {}
    },
    {
        "label": "label_mapping",
        "importPath": "src.processing_pipeline.label_mapping",
        "description": "src.processing_pipeline.label_mapping",
        "isExtraImport": true,
        "detail": "src.processing_pipeline.label_mapping",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "importPath": "src.processing_pipeline.text_processing",
        "description": "src.processing_pipeline.text_processing",
        "isExtraImport": true,
        "detail": "src.processing_pipeline.text_processing",
        "documentation": {}
    },
    {
        "label": "lemmatizer",
        "importPath": "src.processing_pipeline.text_processing",
        "description": "src.processing_pipeline.text_processing",
        "isExtraImport": true,
        "detail": "src.processing_pipeline.text_processing",
        "documentation": {}
    },
    {
        "label": "numericalize",
        "importPath": "src.processing_pipeline.text_processing",
        "description": "src.processing_pipeline.text_processing",
        "isExtraImport": true,
        "detail": "src.processing_pipeline.text_processing",
        "documentation": {}
    },
    {
        "label": "load_model_and_save_to_bento",
        "kind": 2,
        "importPath": "src.model.save_model_to_bento",
        "description": "src.model.save_model_to_bento",
        "peekOfCode": "def load_model_and_save_to_bento(model_file: Path) -> None: \n  lstm_model = IntentClassifier(config).to(device)\n  lstm_model.load_state_dict(torch.load(model_file, map_location=torch.device(device)))\n  bento_model = bentoml.pytorch.save_model('classifier', model=lstm_model)\n  print(f'Bento model tag = {bento_model.tag}')\nif __name__ == '__main__':\n  current_dir = os.path.dirname(os.path.realpath(__file__))\n  model_path = os.path.join(current_dir,  './class_model.pth')\n  print('path', model_path)\n  load_model_and_save_to_bento(Path(model_path))",
        "detail": "src.model.save_model_to_bento",
        "documentation": {}
    },
    {
        "label": "current_dir",
        "kind": 5,
        "importPath": "src.model.save_model_to_bento",
        "description": "src.model.save_model_to_bento",
        "peekOfCode": "current_dir = os.path.dirname(os.path.realpath(__file__))\nfile_path = os.path.join(current_dir,  './config.yaml')\nwith open(file_path, 'r') as f: \n  config = yaml.safe_load(f)\ndevice = 'mps' if torch.cuda.is_available() else 'cpu'\ndef load_model_and_save_to_bento(model_file: Path) -> None: \n  lstm_model = IntentClassifier(config).to(device)\n  lstm_model.load_state_dict(torch.load(model_file, map_location=torch.device(device)))\n  bento_model = bentoml.pytorch.save_model('classifier', model=lstm_model)\n  print(f'Bento model tag = {bento_model.tag}')",
        "detail": "src.model.save_model_to_bento",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "src.model.save_model_to_bento",
        "description": "src.model.save_model_to_bento",
        "peekOfCode": "file_path = os.path.join(current_dir,  './config.yaml')\nwith open(file_path, 'r') as f: \n  config = yaml.safe_load(f)\ndevice = 'mps' if torch.cuda.is_available() else 'cpu'\ndef load_model_and_save_to_bento(model_file: Path) -> None: \n  lstm_model = IntentClassifier(config).to(device)\n  lstm_model.load_state_dict(torch.load(model_file, map_location=torch.device(device)))\n  bento_model = bentoml.pytorch.save_model('classifier', model=lstm_model)\n  print(f'Bento model tag = {bento_model.tag}')\nif __name__ == '__main__':",
        "detail": "src.model.save_model_to_bento",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "src.model.save_model_to_bento",
        "description": "src.model.save_model_to_bento",
        "peekOfCode": "device = 'mps' if torch.cuda.is_available() else 'cpu'\ndef load_model_and_save_to_bento(model_file: Path) -> None: \n  lstm_model = IntentClassifier(config).to(device)\n  lstm_model.load_state_dict(torch.load(model_file, map_location=torch.device(device)))\n  bento_model = bentoml.pytorch.save_model('classifier', model=lstm_model)\n  print(f'Bento model tag = {bento_model.tag}')\nif __name__ == '__main__':\n  current_dir = os.path.dirname(os.path.realpath(__file__))\n  model_path = os.path.join(current_dir,  './class_model.pth')\n  print('path', model_path)",
        "detail": "src.model.save_model_to_bento",
        "documentation": {}
    },
    {
        "label": "label_mapping",
        "kind": 5,
        "importPath": "src.processing_pipeline.label_mapping",
        "description": "src.processing_pipeline.label_mapping",
        "peekOfCode": "label_mapping = {\n    0: 'activate_my_card',\n    1: 'age_limit',\n    2: 'apple_pay_or_google_pay',\n    3: 'atm_support',\n    4: 'automatic_top_up',\n    5: 'balance_not_updated_after_bank_transfer',\n    6: 'balance_not_updated_after_cheque_or_cash_deposit',\n    7: 'beneficiary_not_allowed',\n    8: 'cancel_transfer',",
        "detail": "src.processing_pipeline.label_mapping",
        "documentation": {}
    },
    {
        "label": "clean_text",
        "kind": 2,
        "importPath": "src.processing_pipeline.text_processing",
        "description": "src.processing_pipeline.text_processing",
        "peekOfCode": "def clean_text(text):\n    \"\"\"\n    Descr: Clean text data by removing punctuation, stopwords, and converting to lowercase.\n    Input: text\n    Output: cleaned text\n    \"\"\"\n    tokens = re.sub(r\"\\{\\{.*?\\}\\}\", \"\", text)\n    tokens = word_tokenize(tokens)\n    tokens = [w.lower() for w in tokens]\n    tokens = [word for word in tokens if word.isalpha()]",
        "detail": "src.processing_pipeline.text_processing",
        "documentation": {}
    },
    {
        "label": "nltk_to_wordnet_pos",
        "kind": 2,
        "importPath": "src.processing_pipeline.text_processing",
        "description": "src.processing_pipeline.text_processing",
        "peekOfCode": "def nltk_to_wordnet_pos(nltk_tag):\n  \"\"\"\n  Descr: Convert NLTK POS tag to WordNet POS tag.\n  Input: nltk_tag\n  Output: wordnet_pos\n  \"\"\"\n  if nltk_tag.startswith('J'):\n      return wordnet.ADJ\n  elif nltk_tag.startswith('V'):\n      return wordnet.VERB",
        "detail": "src.processing_pipeline.text_processing",
        "documentation": {}
    },
    {
        "label": "lemmatizer",
        "kind": 2,
        "importPath": "src.processing_pipeline.text_processing",
        "description": "src.processing_pipeline.text_processing",
        "peekOfCode": "def lemmatizer(data):\n  \"\"\"\n  Descr: Lemmatize text data using WordNetLemmatizer.\n  Input: data\n  Output: lemmatized_words\n  \"\"\"\n  lemmatizer = WordNetLemmatizer()\n  tokens = word_tokenize(data)\n  pos_tags = pos_tag(tokens)\n  lemmatized_words = []",
        "detail": "src.processing_pipeline.text_processing",
        "documentation": {}
    },
    {
        "label": "numericalize",
        "kind": 2,
        "importPath": "src.processing_pipeline.text_processing",
        "description": "src.processing_pipeline.text_processing",
        "peekOfCode": "def numericalize(vocab, data):\n    \"\"\"\n    Descr: Numericalize a list of documents or a single document.\n    Input: data - a list of documents or a single document\n           vocab - a dictionary mapping tokens to indices\n    Output: indexed_data - a list of indexed documents or a single indexed document\n    \"\"\"\n    indexed_data = []\n    indexed_seq = [vocab.get(token, vocab['<UNK>']) for token in data]\n    indexed_data.append(indexed_seq)",
        "detail": "src.processing_pipeline.text_processing",
        "documentation": {}
    },
    {
        "label": "IntentClassifier",
        "kind": 6,
        "importPath": "classifier_model",
        "description": "classifier_model",
        "peekOfCode": "class IntentClassifier(nn.Module):\n    def __init__(self, config: dict):\n        super().__init__()\n        self.config = config\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n        self.bi_lstm = nn.LSTM(config['input_size'], config['hidden_size'],\n                             config['num_layers'], dropout=config['dropout_lstm'],\n                             batch_first=True, bidirectional=True)\n        self.dropout_bilstm = nn.Dropout(p=config['dropout_lstm'])\n        self.act1 = nn.GELU()",
        "detail": "classifier_model",
        "documentation": {}
    },
    {
        "label": "current_dir",
        "kind": 5,
        "importPath": "classifier_model",
        "description": "classifier_model",
        "peekOfCode": "current_dir = os.path.dirname(os.path.realpath(__file__))\nfile_path = os.path.join(current_dir, './data/embedding_matrix.pt')\nembedding_matrix = torch.load(file_path)\nclass IntentClassifier(nn.Module):\n    def __init__(self, config: dict):\n        super().__init__()\n        self.config = config\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n        self.bi_lstm = nn.LSTM(config['input_size'], config['hidden_size'],\n                             config['num_layers'], dropout=config['dropout_lstm'],",
        "detail": "classifier_model",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "classifier_model",
        "description": "classifier_model",
        "peekOfCode": "file_path = os.path.join(current_dir, './data/embedding_matrix.pt')\nembedding_matrix = torch.load(file_path)\nclass IntentClassifier(nn.Module):\n    def __init__(self, config: dict):\n        super().__init__()\n        self.config = config\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n        self.bi_lstm = nn.LSTM(config['input_size'], config['hidden_size'],\n                             config['num_layers'], dropout=config['dropout_lstm'],\n                             batch_first=True, bidirectional=True)",
        "detail": "classifier_model",
        "documentation": {}
    },
    {
        "label": "embedding_matrix",
        "kind": 5,
        "importPath": "classifier_model",
        "description": "classifier_model",
        "peekOfCode": "embedding_matrix = torch.load(file_path)\nclass IntentClassifier(nn.Module):\n    def __init__(self, config: dict):\n        super().__init__()\n        self.config = config\n        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n        self.bi_lstm = nn.LSTM(config['input_size'], config['hidden_size'],\n                             config['num_layers'], dropout=config['dropout_lstm'],\n                             batch_first=True, bidirectional=True)\n        self.dropout_bilstm = nn.Dropout(p=config['dropout_lstm'])",
        "detail": "classifier_model",
        "documentation": {}
    },
    {
        "label": "inference",
        "kind": 2,
        "importPath": "service",
        "description": "service",
        "peekOfCode": "def inference(text: str):\n  cleaned_text = clean_text(text)\n  lemmatized_text = lemmatizer(cleaned_text)\n  numericalized_text = numericalize(vocab, lemmatized_text)\n  tensor_text = torch.tensor(numericalized_text)\n  with torch.no_grad():\n    pred = classifier.run(tensor_text)\n    pred = torch.argmax(pred, dim=1).cpu().numpy()\n    predicted_label = label_mapping[pred[0]]\n    print(\"Predicted_label\", predicted_label)",
        "detail": "service",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "service",
        "description": "service",
        "peekOfCode": "device = 'mps' if torch.cuda.is_available() else 'cpu'\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\nvocab_path = os.path.join(current_dir, './data/vocab.json')\nwith open(vocab_path, 'r') as f:\n    vocab = json.load(f)\nclassifier = bentoml.pytorch.get('classifier:latest').to_runner()\nsvc = bentoml.Service('classifier', runners=[classifier])\n@svc.api(input=Text(), output=Text()) \ndef inference(text: str):\n  cleaned_text = clean_text(text)",
        "detail": "service",
        "documentation": {}
    },
    {
        "label": "current_dir",
        "kind": 5,
        "importPath": "service",
        "description": "service",
        "peekOfCode": "current_dir = os.path.dirname(os.path.realpath(__file__))\nvocab_path = os.path.join(current_dir, './data/vocab.json')\nwith open(vocab_path, 'r') as f:\n    vocab = json.load(f)\nclassifier = bentoml.pytorch.get('classifier:latest').to_runner()\nsvc = bentoml.Service('classifier', runners=[classifier])\n@svc.api(input=Text(), output=Text()) \ndef inference(text: str):\n  cleaned_text = clean_text(text)\n  lemmatized_text = lemmatizer(cleaned_text)",
        "detail": "service",
        "documentation": {}
    },
    {
        "label": "vocab_path",
        "kind": 5,
        "importPath": "service",
        "description": "service",
        "peekOfCode": "vocab_path = os.path.join(current_dir, './data/vocab.json')\nwith open(vocab_path, 'r') as f:\n    vocab = json.load(f)\nclassifier = bentoml.pytorch.get('classifier:latest').to_runner()\nsvc = bentoml.Service('classifier', runners=[classifier])\n@svc.api(input=Text(), output=Text()) \ndef inference(text: str):\n  cleaned_text = clean_text(text)\n  lemmatized_text = lemmatizer(cleaned_text)\n  numericalized_text = numericalize(vocab, lemmatized_text)",
        "detail": "service",
        "documentation": {}
    },
    {
        "label": "classifier",
        "kind": 5,
        "importPath": "service",
        "description": "service",
        "peekOfCode": "classifier = bentoml.pytorch.get('classifier:latest').to_runner()\nsvc = bentoml.Service('classifier', runners=[classifier])\n@svc.api(input=Text(), output=Text()) \ndef inference(text: str):\n  cleaned_text = clean_text(text)\n  lemmatized_text = lemmatizer(cleaned_text)\n  numericalized_text = numericalize(vocab, lemmatized_text)\n  tensor_text = torch.tensor(numericalized_text)\n  with torch.no_grad():\n    pred = classifier.run(tensor_text)",
        "detail": "service",
        "documentation": {}
    },
    {
        "label": "svc",
        "kind": 5,
        "importPath": "service",
        "description": "service",
        "peekOfCode": "svc = bentoml.Service('classifier', runners=[classifier])\n@svc.api(input=Text(), output=Text()) \ndef inference(text: str):\n  cleaned_text = clean_text(text)\n  lemmatized_text = lemmatizer(cleaned_text)\n  numericalized_text = numericalize(vocab, lemmatized_text)\n  tensor_text = torch.tensor(numericalized_text)\n  with torch.no_grad():\n    pred = classifier.run(tensor_text)\n    pred = torch.argmax(pred, dim=1).cpu().numpy()",
        "detail": "service",
        "documentation": {}
    }
]